{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>Class 1</th>\n",
       "      <th>Class 2</th>\n",
       "      <th>Class 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  1.518613 -0.562250  0.232053 -1.169593  1.913905  0.808997  1.034819   \n",
       "1  0.246290 -0.499413 -0.827996 -2.490847  0.018145  0.568648  0.733629   \n",
       "2  0.196879  0.021231  1.109334 -0.268738  0.088358  0.808997  1.215533   \n",
       "3  1.691550 -0.346811  0.487926 -0.809251  0.930918  2.491446  1.466525   \n",
       "4  0.295700  0.227694  1.840403  0.451946  1.281985  0.808997  0.663351   \n",
       "\n",
       "          8         9        10        11        12        13  Class 1  \\\n",
       "0 -0.659563  1.224884  0.251717  0.362177  1.847920  1.013009        1   \n",
       "1 -0.820719 -0.544721 -0.293321  0.406051  1.113449  0.965242        1   \n",
       "2 -0.498407  2.135968  0.269020  0.318304  0.788587  1.395148        1   \n",
       "3 -0.981875  1.032155  1.186068 -0.427544  1.184071  2.334574        1   \n",
       "4  0.226796  0.401404 -0.319276  0.362177  0.449601 -0.037874        1   \n",
       "\n",
       "   Class 2  Class 3  \n",
       "0        0        0  \n",
       "1        0        0  \n",
       "2        0        0  \n",
       "3        0        0  \n",
       "4        0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./Drinks.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get the wine labels\n",
    "y = df[['Class 1', 'Class 2', 'Class 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "X = df[['1','2','3','4','5','6','7','8','9','10','11','12','13']].values\n",
    "\n",
    "print(X.shape, y.shape) # Print shapes just to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x) \n",
    "    return exps / np.sum(exps,axis=1,keepdims=True)\n",
    "\n",
    "def softmax_grad(softmax):\n",
    "    a = softmax.reshape(-1,1);\n",
    "    return np.diagflat(a) - np.dot(a, a.T)\n",
    "\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    minval = 0.0000001\n",
    "    m = y.shape[0]\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "#     return (y_hat-y) * softmax_grad(softmax(y_hat))\n",
    "# we need to compute softmax_derivative too and multiply to loss_derivative function.\n",
    "    return (y_hat - y)\n",
    "\n",
    "\n",
    "def feed_forward(model,a0):\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']    \n",
    "    z1 = a0.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = softmax(z3)\n",
    "    params = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return params\n",
    "\n",
    "\n",
    "def backward_prop(model,params,y):\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    a0,a1,a2,a3 = params['a0'],params['a1'],params['a2'],params['a3']\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    #loss derivative with respect to layer 2 weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) \n",
    "\n",
    "    # loss derivative with respect to 2 layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    #loss derivative with respect to layer 1\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to layer 1 weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to layer 1  bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "\n",
    "def initialize_parameters(input_dim,hidden_dim,output_dim):\n",
    "    \n",
    "    W1 = 2 *np.random.randn(input_dim, hidden_dim) - 1\n",
    "    \n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    \n",
    "    W2 = 2 * np.random.randn(hidden_dim, hidden_dim) - 1\n",
    "    \n",
    "    b2 = np.zeros((1, hidden_dim))\n",
    "    W3 = 2 * np.random.rand(hidden_dim, output_dim) - 1\n",
    "    b3 = np.zeros((1,output_dim))\n",
    "    \n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, x):\n",
    "    c = feed_forward(model,x)\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def calc_accuracy(model,x,y):\n",
    "    m = y.shape[0]\n",
    "    pred = predict(model,x)\n",
    "    pred = pred.reshape(y.shape)\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "\n",
    "\n",
    "\n",
    "def train(model,X_,y_,learning_rate, epochs=20000):\n",
    "    for i in range(0, epochs):\n",
    "        params = feed_forward(model,X_)        \n",
    "        grads = backward_prop(model,params,y_)\n",
    "        W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "       \n",
    "        W1 -= learning_rate * grads['dW1']\n",
    "        b1 -= learning_rate * grads['db1']\n",
    "        W2 -= learning_rate * grads['dW2']\n",
    "        b2 -= learning_rate * grads['db2']\n",
    "        W3 -= learning_rate * grads['dW3']\n",
    "        b3 -= learning_rate * grads['db3']\n",
    "\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "        a3 = params['a3']\n",
    "        print('Loss in iteration',i,':',softmax_loss(y_,a3))\n",
    "        y_hat = predict(model,X_)\n",
    "        y_true = y_.argmax(axis=1)\n",
    "        print('Accuracy in iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "        losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iteration 0 : 1.4504813381979211\n",
      "Accuracy in iteration 0 : 25.280898876404496 %\n",
      "Loss in iteration 1 : 1.3848066834768182\n",
      "Accuracy in iteration 1 : 26.40449438202247 %\n",
      "Loss in iteration 2 : 1.3260266812954302\n",
      "Accuracy in iteration 2 : 26.40449438202247 %\n",
      "Loss in iteration 3 : 1.273418157219208\n",
      "Accuracy in iteration 3 : 26.96629213483146 %\n",
      "Loss in iteration 4 : 1.2263067146919435\n",
      "Accuracy in iteration 4 : 27.52808988764045 %\n",
      "Loss in iteration 5 : 1.1840710478981928\n",
      "Accuracy in iteration 5 : 29.775280898876407 %\n",
      "Loss in iteration 6 : 1.1461450432017364\n",
      "Accuracy in iteration 6 : 30.89887640449438 %\n",
      "Loss in iteration 7 : 1.112017905445678\n",
      "Accuracy in iteration 7 : 44.38202247191011 %\n",
      "Loss in iteration 8 : 1.0812326561780647\n",
      "Accuracy in iteration 8 : 46.62921348314607 %\n",
      "Loss in iteration 9 : 1.0533834016889774\n",
      "Accuracy in iteration 9 : 51.12359550561798 %\n",
      "Loss in iteration 10 : 1.0281117672282296\n",
      "Accuracy in iteration 10 : 53.37078651685393 %\n",
      "Loss in iteration 11 : 1.0051028523932495\n",
      "Accuracy in iteration 11 : 53.93258426966292 %\n",
      "Loss in iteration 12 : 0.9840809980281803\n",
      "Accuracy in iteration 12 : 53.93258426966292 %\n",
      "Loss in iteration 13 : 0.9648055830422716\n",
      "Accuracy in iteration 13 : 53.93258426966292 %\n",
      "Loss in iteration 14 : 0.9470670020317924\n",
      "Accuracy in iteration 14 : 53.93258426966292 %\n",
      "Loss in iteration 15 : 0.9306829178842091\n",
      "Accuracy in iteration 15 : 53.93258426966292 %\n",
      "Loss in iteration 16 : 0.9154948397224055\n",
      "Accuracy in iteration 16 : 53.93258426966292 %\n",
      "Loss in iteration 17 : 0.901365044937426\n",
      "Accuracy in iteration 17 : 53.93258426966292 %\n",
      "Loss in iteration 18 : 0.8881738426806265\n",
      "Accuracy in iteration 18 : 54.49438202247191 %\n",
      "Loss in iteration 19 : 0.8758171627160926\n",
      "Accuracy in iteration 19 : 62.92134831460674 %\n",
      "Loss in iteration 20 : 0.8642044458215353\n",
      "Accuracy in iteration 20 : 63.48314606741573 %\n",
      "Loss in iteration 21 : 0.8532568082032251\n",
      "Accuracy in iteration 21 : 63.48314606741573 %\n",
      "Loss in iteration 22 : 0.8429054513066109\n",
      "Accuracy in iteration 22 : 64.60674157303372 %\n",
      "Loss in iteration 23 : 0.8330902889763668\n",
      "Accuracy in iteration 23 : 65.1685393258427 %\n",
      "Loss in iteration 24 : 0.8237587654600903\n",
      "Accuracy in iteration 24 : 65.1685393258427 %\n",
      "Loss in iteration 25 : 0.8148648397940292\n",
      "Accuracy in iteration 25 : 66.29213483146067 %\n",
      "Loss in iteration 26 : 0.8063681143548767\n",
      "Accuracy in iteration 26 : 66.29213483146067 %\n",
      "Loss in iteration 27 : 0.7982330876226489\n",
      "Accuracy in iteration 27 : 66.29213483146067 %\n",
      "Loss in iteration 28 : 0.7904285133713173\n",
      "Accuracy in iteration 28 : 66.29213483146067 %\n",
      "Loss in iteration 29 : 0.7829268505379153\n",
      "Accuracy in iteration 29 : 66.29213483146067 %\n",
      "Loss in iteration 30 : 0.7757037899065051\n",
      "Accuracy in iteration 30 : 66.29213483146067 %\n",
      "Loss in iteration 31 : 0.7687378454924044\n",
      "Accuracy in iteration 31 : 66.85393258426966 %\n",
      "Loss in iteration 32 : 0.7620100001463052\n",
      "Accuracy in iteration 32 : 66.85393258426966 %\n",
      "Loss in iteration 33 : 0.7555033964385646\n",
      "Accuracy in iteration 33 : 67.41573033707866 %\n",
      "Loss in iteration 34 : 0.7492030653431326\n",
      "Accuracy in iteration 34 : 67.97752808988764 %\n",
      "Loss in iteration 35 : 0.7430956866158712\n",
      "Accuracy in iteration 35 : 70.78651685393258 %\n",
      "Loss in iteration 36 : 0.7371693760352157\n",
      "Accuracy in iteration 36 : 70.78651685393258 %\n",
      "Loss in iteration 37 : 0.731413495813644\n",
      "Accuracy in iteration 37 : 70.78651685393258 %\n",
      "Loss in iteration 38 : 0.7258184854609144\n",
      "Accuracy in iteration 38 : 71.91011235955057 %\n",
      "Loss in iteration 39 : 0.7203757111540859\n",
      "Accuracy in iteration 39 : 71.91011235955057 %\n",
      "Loss in iteration 40 : 0.715077332228787\n",
      "Accuracy in iteration 40 : 73.59550561797754 %\n",
      "Loss in iteration 41 : 0.7099161837551501\n",
      "Accuracy in iteration 41 : 75.84269662921348 %\n",
      "Loss in iteration 42 : 0.7048856743255637\n",
      "Accuracy in iteration 42 : 75.84269662921348 %\n",
      "Loss in iteration 43 : 0.699979698202218\n",
      "Accuracy in iteration 43 : 75.84269662921348 %\n",
      "Loss in iteration 44 : 0.6951925609017454\n",
      "Accuracy in iteration 44 : 75.84269662921348 %\n",
      "Loss in iteration 45 : 0.6905189171833991\n",
      "Accuracy in iteration 45 : 75.28089887640449 %\n",
      "Loss in iteration 46 : 0.6859537202995739\n",
      "Accuracy in iteration 46 : 75.28089887640449 %\n",
      "Loss in iteration 47 : 0.6814921812935092\n",
      "Accuracy in iteration 47 : 75.28089887640449 %\n",
      "Loss in iteration 48 : 0.6771297371047765\n",
      "Accuracy in iteration 48 : 75.28089887640449 %\n",
      "Loss in iteration 49 : 0.6728620262717631\n",
      "Accuracy in iteration 49 : 75.28089887640449 %\n",
      "Loss in iteration 50 : 0.6686848710952311\n",
      "Accuracy in iteration 50 : 75.84269662921348 %\n",
      "Loss in iteration 51 : 0.6645942652358037\n",
      "Accuracy in iteration 51 : 75.84269662921348 %\n",
      "Loss in iteration 52 : 0.6605863658461872\n",
      "Accuracy in iteration 52 : 75.84269662921348 %\n",
      "Loss in iteration 53 : 0.6566574894715911\n",
      "Accuracy in iteration 53 : 75.84269662921348 %\n",
      "Loss in iteration 54 : 0.6528041110763168\n",
      "Accuracy in iteration 54 : 76.40449438202246 %\n",
      "Loss in iteration 55 : 0.64902286566026\n",
      "Accuracy in iteration 55 : 76.40449438202246 %\n",
      "Loss in iteration 56 : 0.6453105520076402\n",
      "Accuracy in iteration 56 : 76.40449438202246 %\n",
      "Loss in iteration 57 : 0.6416641381551189\n",
      "Accuracy in iteration 57 : 76.40449438202246 %\n",
      "Loss in iteration 58 : 0.6380807681727368\n",
      "Accuracy in iteration 58 : 76.96629213483146 %\n",
      "Loss in iteration 59 : 0.6345577698160478\n",
      "Accuracy in iteration 59 : 77.52808988764045 %\n",
      "Loss in iteration 60 : 0.6310926625316055\n",
      "Accuracy in iteration 60 : 77.52808988764045 %\n",
      "Loss in iteration 61 : 0.6276831651853403\n",
      "Accuracy in iteration 61 : 78.08988764044943 %\n",
      "Loss in iteration 62 : 0.6243272027458254\n",
      "Accuracy in iteration 62 : 78.08988764044943 %\n",
      "Loss in iteration 63 : 0.6210229110121356\n",
      "Accuracy in iteration 63 : 78.65168539325843 %\n",
      "Loss in iteration 64 : 0.6177686383588153\n",
      "Accuracy in iteration 64 : 78.08988764044943 %\n",
      "Loss in iteration 65 : 0.614562943416462\n",
      "Accuracy in iteration 65 : 78.08988764044943 %\n",
      "Loss in iteration 66 : 0.6114045876569209\n",
      "Accuracy in iteration 66 : 78.08988764044943 %\n",
      "Loss in iteration 67 : 0.6082925220425995\n",
      "Accuracy in iteration 67 : 78.08988764044943 %\n",
      "Loss in iteration 68 : 0.6052258672478298\n",
      "Accuracy in iteration 68 : 78.08988764044943 %\n",
      "Loss in iteration 69 : 0.6022038874550502\n",
      "Accuracy in iteration 69 : 78.08988764044943 %\n",
      "Loss in iteration 70 : 0.5992259583224492\n",
      "Accuracy in iteration 70 : 78.08988764044943 %\n",
      "Loss in iteration 71 : 0.5962915303319978\n",
      "Accuracy in iteration 71 : 78.08988764044943 %\n",
      "Loss in iteration 72 : 0.593400089258141\n",
      "Accuracy in iteration 72 : 78.65168539325843 %\n",
      "Loss in iteration 73 : 0.5905511158529819\n",
      "Accuracy in iteration 73 : 79.21348314606742 %\n",
      "Loss in iteration 74 : 0.587744046958527\n",
      "Accuracy in iteration 74 : 79.21348314606742 %\n",
      "Loss in iteration 75 : 0.5849782401135969\n",
      "Accuracy in iteration 75 : 79.21348314606742 %\n",
      "Loss in iteration 76 : 0.5822529433582254\n",
      "Accuracy in iteration 76 : 79.21348314606742 %\n",
      "Loss in iteration 77 : 0.5795672714303559\n",
      "Accuracy in iteration 77 : 79.21348314606742 %\n",
      "Loss in iteration 78 : 0.5769201889989365\n",
      "Accuracy in iteration 78 : 79.7752808988764 %\n",
      "Loss in iteration 79 : 0.574310501083712\n",
      "Accuracy in iteration 79 : 79.7752808988764 %\n",
      "Loss in iteration 80 : 0.5717368504552313\n",
      "Accuracy in iteration 80 : 79.7752808988764 %\n",
      "Loss in iteration 81 : 0.5691977216401141\n",
      "Accuracy in iteration 81 : 79.7752808988764 %\n",
      "Loss in iteration 82 : 0.5666914511977249\n",
      "Accuracy in iteration 82 : 79.7752808988764 %\n",
      "Loss in iteration 83 : 0.5642162441809094\n",
      "Accuracy in iteration 83 : 79.7752808988764 %\n",
      "Loss in iteration 84 : 0.5617701971210503\n",
      "Accuracy in iteration 84 : 79.7752808988764 %\n",
      "Loss in iteration 85 : 0.5593513284436902\n",
      "Accuracy in iteration 85 : 79.7752808988764 %\n",
      "Loss in iteration 86 : 0.5569576178589213\n",
      "Accuracy in iteration 86 : 79.7752808988764 %\n",
      "Loss in iteration 87 : 0.5545870568753494\n",
      "Accuracy in iteration 87 : 79.7752808988764 %\n",
      "Loss in iteration 88 : 0.5522377129885334\n",
      "Accuracy in iteration 88 : 80.33707865168539 %\n",
      "Loss in iteration 89 : 0.5499078100308342\n",
      "Accuracy in iteration 89 : 80.33707865168539 %\n",
      "Loss in iteration 90 : 0.5475958262594315\n",
      "Accuracy in iteration 90 : 80.33707865168539 %\n",
      "Loss in iteration 91 : 0.5453006095230667\n",
      "Accuracy in iteration 91 : 80.33707865168539 %\n",
      "Loss in iteration 92 : 0.5430215048141455\n",
      "Accuracy in iteration 92 : 80.33707865168539 %\n",
      "Loss in iteration 93 : 0.540758483480112\n",
      "Accuracy in iteration 93 : 80.33707865168539 %\n",
      "Loss in iteration 94 : 0.5385122558536808\n",
      "Accuracy in iteration 94 : 80.33707865168539 %\n",
      "Loss in iteration 95 : 0.536284341831341\n",
      "Accuracy in iteration 95 : 80.33707865168539 %\n",
      "Loss in iteration 96 : 0.5340770702555683\n",
      "Accuracy in iteration 96 : 80.33707865168539 %\n",
      "Loss in iteration 97 : 0.5318934819945704\n",
      "Accuracy in iteration 97 : 80.33707865168539 %\n",
      "Loss in iteration 98 : 0.5297371264134669\n",
      "Accuracy in iteration 98 : 80.33707865168539 %\n",
      "Loss in iteration 99 : 0.5276117655010016\n",
      "Accuracy in iteration 99 : 80.89887640449437 %\n",
      "Loss in iteration 100 : 0.5255210272876467\n",
      "Accuracy in iteration 100 : 80.89887640449437 %\n",
      "Loss in iteration 101 : 0.52346806907767\n",
      "Accuracy in iteration 101 : 80.89887640449437 %\n",
      "Loss in iteration 102 : 0.5214553116589271\n",
      "Accuracy in iteration 102 : 80.89887640449437 %\n",
      "Loss in iteration 103 : 0.5194842863773254\n",
      "Accuracy in iteration 103 : 80.89887640449437 %\n",
      "Loss in iteration 104 : 0.5175556060038622\n",
      "Accuracy in iteration 104 : 81.46067415730337 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iteration 105 : 0.5156690408490157\n",
      "Accuracy in iteration 105 : 81.46067415730337 %\n",
      "Loss in iteration 106 : 0.5138236638652977\n",
      "Accuracy in iteration 106 : 81.46067415730337 %\n",
      "Loss in iteration 107 : 0.5120180252775246\n",
      "Accuracy in iteration 107 : 81.46067415730337 %\n",
      "Loss in iteration 108 : 0.5102503247763267\n",
      "Accuracy in iteration 108 : 81.46067415730337 %\n",
      "Loss in iteration 109 : 0.5085185612398999\n",
      "Accuracy in iteration 109 : 81.46067415730337 %\n",
      "Loss in iteration 110 : 0.5068206512227375\n",
      "Accuracy in iteration 110 : 81.46067415730337 %\n",
      "Loss in iteration 111 : 0.5051545155452369\n",
      "Accuracy in iteration 111 : 82.02247191011236 %\n",
      "Loss in iteration 112 : 0.5035181378738427\n",
      "Accuracy in iteration 112 : 82.02247191011236 %\n",
      "Loss in iteration 113 : 0.5019096009667317\n",
      "Accuracy in iteration 113 : 82.02247191011236 %\n",
      "Loss in iteration 114 : 0.5003271063179524\n",
      "Accuracy in iteration 114 : 82.02247191011236 %\n",
      "Loss in iteration 115 : 0.4987689821343634\n",
      "Accuracy in iteration 115 : 82.02247191011236 %\n",
      "Loss in iteration 116 : 0.4972336834983134\n",
      "Accuracy in iteration 116 : 82.02247191011236 %\n",
      "Loss in iteration 117 : 0.4957197875174971\n",
      "Accuracy in iteration 117 : 82.02247191011236 %\n",
      "Loss in iteration 118 : 0.4942259853773238\n",
      "Accuracy in iteration 118 : 82.02247191011236 %\n",
      "Loss in iteration 119 : 0.49275107252498773\n",
      "Accuracy in iteration 119 : 82.02247191011236 %\n",
      "Loss in iteration 120 : 0.4912939377143427\n",
      "Accuracy in iteration 120 : 82.02247191011236 %\n",
      "Loss in iteration 121 : 0.4898535512950989\n",
      "Accuracy in iteration 121 : 82.02247191011236 %\n",
      "Loss in iteration 122 : 0.4884289529049042\n",
      "Accuracy in iteration 122 : 82.02247191011236 %\n",
      "Loss in iteration 123 : 0.4870192385886714\n",
      "Accuracy in iteration 123 : 82.02247191011236 %\n",
      "Loss in iteration 124 : 0.48562354730172097\n",
      "Accuracy in iteration 124 : 82.02247191011236 %\n",
      "Loss in iteration 125 : 0.4842410467327066\n",
      "Accuracy in iteration 125 : 82.02247191011236 %\n",
      "Loss in iteration 126 : 0.4828709183936532\n",
      "Accuracy in iteration 126 : 82.02247191011236 %\n",
      "Loss in iteration 127 : 0.4815123419556696\n",
      "Accuracy in iteration 127 : 82.02247191011236 %\n",
      "Loss in iteration 128 : 0.4801644788506212\n",
      "Accuracy in iteration 128 : 82.02247191011236 %\n",
      "Loss in iteration 129 : 0.47882645520461575\n",
      "Accuracy in iteration 129 : 82.02247191011236 %\n",
      "Loss in iteration 130 : 0.47749734421522877\n",
      "Accuracy in iteration 130 : 82.02247191011236 %\n",
      "Loss in iteration 131 : 0.47617614813159476\n",
      "Accuracy in iteration 131 : 82.58426966292134 %\n",
      "Loss in iteration 132 : 0.47486178005008345\n",
      "Accuracy in iteration 132 : 83.14606741573034 %\n",
      "Loss in iteration 133 : 0.4735530458084311\n",
      "Accuracy in iteration 133 : 83.14606741573034 %\n",
      "Loss in iteration 134 : 0.47224862636254855\n",
      "Accuracy in iteration 134 : 83.14606741573034 %\n",
      "Loss in iteration 135 : 0.4709470611803309\n",
      "Accuracy in iteration 135 : 83.14606741573034 %\n",
      "Loss in iteration 136 : 0.46964673340300983\n",
      "Accuracy in iteration 136 : 83.14606741573034 %\n",
      "Loss in iteration 137 : 0.46834585781883936\n",
      "Accuracy in iteration 137 : 83.14606741573034 %\n",
      "Loss in iteration 138 : 0.4670424730648911\n",
      "Accuracy in iteration 138 : 83.14606741573034 %\n",
      "Loss in iteration 139 : 0.4657344398957082\n",
      "Accuracy in iteration 139 : 83.70786516853933 %\n",
      "Loss in iteration 140 : 0.4644194477735013\n",
      "Accuracy in iteration 140 : 83.70786516853933 %\n",
      "Loss in iteration 141 : 0.46309503234405514\n",
      "Accuracy in iteration 141 : 83.70786516853933 %\n",
      "Loss in iteration 142 : 0.46175860643532546\n",
      "Accuracy in iteration 142 : 83.70786516853933 %\n",
      "Loss in iteration 143 : 0.4604075069332887\n",
      "Accuracy in iteration 143 : 83.70786516853933 %\n",
      "Loss in iteration 144 : 0.4590390592385265\n",
      "Accuracy in iteration 144 : 83.70786516853933 %\n",
      "Loss in iteration 145 : 0.4576506602243139\n",
      "Accuracy in iteration 145 : 84.26966292134831 %\n",
      "Loss in iteration 146 : 0.4562398803350266\n",
      "Accuracy in iteration 146 : 84.26966292134831 %\n",
      "Loss in iteration 147 : 0.4548045867041706\n",
      "Accuracy in iteration 147 : 84.26966292134831 %\n",
      "Loss in iteration 148 : 0.453343092927384\n",
      "Accuracy in iteration 148 : 84.26966292134831 %\n",
      "Loss in iteration 149 : 0.45185434726340223\n",
      "Accuracy in iteration 149 : 84.8314606741573 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1693697b8>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "model = initialize_parameters(13,5,3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=150)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
